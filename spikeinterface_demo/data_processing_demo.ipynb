{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Data Processing Demo\n",
    "\n",
    "This demo shows a comprehensive data processing pipeline using SpikeInterface, including:\n",
    "1. Loading and preprocessing data\n",
    "2. Computing quality metrics\n",
    "3. Automated curation using pre-trained models\n",
    "4. Visualization of results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spikeinterface.core as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.curation as sc\n",
    "import spikeinterface.widgets as sw\n",
    "import spikeinterface.sortingcomponents as sc_sort\n",
    "from spikeinterface.sortingcomponents.peak_detection import DetectPeakLocallyExclusive\n",
    "from spikeinterface.postprocessing import compute_principal_components\n",
    "from spikeinterface.qualitymetrics import (\n",
    "    compute_snrs,\n",
    "    compute_firing_rates,\n",
    "    compute_isi_violations,\n",
    "    calculate_pc_metrics,\n",
    "    compute_quality_metrics,\n",
    ")\n",
    "from probeinterface import Probe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Loading and Preprocessing Data\n",
    "\n",
    "First, we'll load a Blackrock dataset and preprocess it for analysis. We'll handle single-channel data properly by reshaping as needed and setting up appropriate probe information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Blackrock dataset...\n",
      "Channel IDs: ['6']\n",
      "Recording shape: (900210,) (using channel_ids=[6])\n",
      "Reshaped to 2D: (900210, 1)\n",
      "Created NumpyRecording: NumpyRecording: 1 channels - 30.0kHz - 1 segments - 900,210 samples - 30.01s - int16 dtype \n",
      "                1.72 MiB\n",
      "Sampling frequency: 30000.0 Hz\n",
      "Number of channels: 1\n",
      "Duration: 30.007 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Blackrock dataset...\")\n",
    "# Use SpikeInterface's Blackrock extractor which handles the data properly\n",
    "recording = se.BlackrockRecordingExtractor('/Users/nielsnovotny/Downloads/Hub1-datafile001.ns6')\n",
    "channel_ids = recording.get_channel_ids()\n",
    "print(f\"Channel IDs: {channel_ids}\")\n",
    "traces = recording.get_traces(channel_ids=[channel_ids[0]])\n",
    "print(f\"Recording shape: {traces.shape} (using channel_ids=[{channel_ids[0]}])\")\n",
    "if traces.ndim == 1:\n",
    "    traces = traces.reshape(-1, 1)\n",
    "    print(f\"Reshaped to 2D: {traces.shape}\")\n",
    "# Use NumpyRecording to ensure compatibility with single-channel data\n",
    "from spikeinterface.core import NumpyRecording\n",
    "recording = NumpyRecording(\n",
    "    traces,\n",
    "    sampling_frequency=recording.get_sampling_frequency(),\n",
    "    channel_ids=[str(channel_ids[0])]\n",
    ")\n",
    "# Add gain information for scaling (assuming 1.0 gain for demo purposes)\n",
    "recording.set_property(\"gain_to_uV\", [1.0])\n",
    "recording.set_property(\"offset_to_uV\", [0.0])\n",
    "print(f\"Created NumpyRecording: {recording}\")\n",
    "print(f\"Sampling frequency: {recording.get_sampling_frequency()} Hz\")\n",
    "print(f\"Number of channels: {recording.get_num_channels()}\")\n",
    "print(f\"Duration: {recording.get_total_duration()} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Setting up the Probe\n",
    "\n",
    "Create and attach a probe object to our recording. For single-channel data, we'll create a simple probe with one contact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe attached: Probe - 1ch - 1shanks\n"
     ]
    }
   ],
   "source": [
    "# Create a probe with 1 shank and 1 channel\n",
    "probe = Probe(ndim=2)\n",
    "probe.set_contacts(\n",
    "    positions=np.array([[0, 0]]),  # Single contact at origin\n",
    "    shapes='circle',\n",
    "    shape_params={'radius': 5},  # 5 micron radius\n",
    "    shank_ids=[0]  # All contacts on shank 0\n",
    ")\n",
    "probe.set_device_channel_indices([0])  # Map channel 0 to contact 0\n",
    "\n",
    "# Attach the probe to the recording\n",
    "recording.set_probe(probe, in_place=True)\n",
    "print(f\"Probe attached: {recording.get_probe()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Spike Detection\n",
    "\n",
    "We'll use threshold-based spike detection to identify potential spikes in our recording.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 63316 peaks\n",
      "Created sorting with 1 units\n"
     ]
    }
   ],
   "source": [
    "# Detect spikes using threshold detection\n",
    "# Prepare arguments for detect_peaks\n",
    "traces = recording.get_traces()\n",
    "peak_sign = \"neg\"\n",
    "abs_thresholds = np.array([5])  # threshold in standard deviations, one per channel\n",
    "exclude_sweep_size = int(0.1 * recording.get_sampling_frequency() / 1000)  # 0.1 ms in samples\n",
    "neighbours_mask = np.ones((1, 1), dtype=bool)  # For single channel, allow self-neighbor\n",
    "\n",
    "peaks = DetectPeakLocallyExclusive.detect_peaks(\n",
    "    traces, peak_sign, abs_thresholds, exclude_sweep_size, neighbours_mask\n",
    ")\n",
    "print(f\"Detected {len(peaks[0])} peaks\")\n",
    "\n",
    "# peaks is a tuple: (sample_indices, channel_indices)\n",
    "peaks_struct = np.zeros(\n",
    "    len(peaks[0]),\n",
    "    dtype=[(\"sample_index\", \"int64\"), (\"channel_index\", \"int64\"), (\"segment_index\", \"int64\")]\n",
    ")\n",
    "peaks_struct[\"sample_index\"] = peaks[0]\n",
    "peaks_struct[\"channel_index\"] = peaks[1]\n",
    "peaks_struct[\"segment_index\"] = 0  # All zeros for single segment\n",
    "\n",
    "# Convert peaks to sorting\n",
    "from spikeinterface.core import NumpySorting\n",
    "sorting = NumpySorting.from_peaks(\n",
    "    peaks_struct,\n",
    "    sampling_frequency=recording.get_sampling_frequency(),\n",
    "    unit_ids=[\"unit_0\"]  # Single unit for all peaks\n",
    ")\n",
    "print(f\"Created sorting with {len(sorting.get_unit_ids())} units\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Creating SortingAnalyzer and Computing Extensions\n",
    "\n",
    "The SortingAnalyzer is a powerful object that allows us to compute and store various metrics and features of our sorted data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating SortingAnalyzer and computing extensions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e89d5068654f7998ce7c3ae7dd4154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "estimate_sparsity (no parallelization):   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d289297b91a14b7589b4dc50630f2147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compute_waveforms (workers: 2 processes):   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7f51a136294e2c950fbc5fc55b8a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "noise_level (no parallelization):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e697f234b4942f2b1855f22db44f1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fitting PCA:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e7d31ae68e4826ba26daf74d6275b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Projecting waveforms:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe39276a56b4d9f94ba3c65aa8695a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spike_locations (no parallelization):   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0e70322af94580a9cc135916e879ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spike_amplitudes (no parallelization):   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d04b18f5f4443f39330cd2a6c2f0685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "noise_level (no parallelization):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae214e254e4541beab42ce076347bd6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "calculate pc_metrics:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer: SortingAnalyzer: 1 channels - 1 units - 1 segments - memory - sparse - has recording\n",
      "Loaded 10 extensions: random_spikes, waveforms, templates, noise_levels, principal_components, spike_locations, spike_amplitudes, correlograms, quality_metrics, template_metrics\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreating SortingAnalyzer and computing extensions...\")\n",
    "analyzer = si.create_sorting_analyzer(\n",
    "    sorting=sorting,\n",
    "    recording=recording,\n",
    "    format=\"memory\"  # Store in memory for faster access\n",
    ")\n",
    "\n",
    "# Compute necessary extensions for quality metrics\n",
    "analyzer.compute(\"random_spikes\", method=\"uniform\", max_spikes_per_unit=600, seed=2205)\n",
    "analyzer.compute(\"waveforms\", ms_before=1.3, ms_after=2.6, n_jobs=2)\n",
    "analyzer.compute(\"templates\", operators=[\"average\", \"median\", \"std\"])\n",
    "analyzer.compute(\"noise_levels\")\n",
    "analyzer.compute(\"principal_components\", n_components=3, mode=\"by_channel_global\", whiten=True)\n",
    "analyzer.compute(\"spike_locations\")\n",
    "analyzer.compute(\"spike_amplitudes\")\n",
    "analyzer.compute(\"correlograms\")\n",
    "analyzer.compute(\"quality_metrics\")\n",
    "# Compute all required template metrics for model-based curation\n",
    "required_metrics = [\n",
    "    'half_width', 'num_negative_peaks', 'peak_to_valley', 'num_positive_peaks',\n",
    "    'peak_trough_ratio', 'recovery_slope', 'repolarization_slope'\n",
    "]\n",
    "analyzer.compute('template_metrics', metric_names=required_metrics)\n",
    "\n",
    "print(f\"Analyzer: {analyzer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Computing Quality Metrics\n",
    "\n",
    "We'll compute various quality metrics to assess the quality of our sorted units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComputing quality metrics...\")\n",
    "# Compute individual metrics\n",
    "firing_rates = compute_firing_rates(analyzer)\n",
    "snrs = compute_snrs(analyzer)\n",
    "\n",
    "# Compute comprehensive metrics\n",
    "metrics = compute_quality_metrics(\n",
    "    analyzer,\n",
    "    metric_names=[\n",
    "        \"firing_rate\",\n",
    "        \"snr\",\n",
    "        \"amplitude_cutoff\",\n",
    "        \"firing_range\",\n",
    "        \"isolation_distance\",\n",
    "        \"d_prime\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\nQuality metrics summary:\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Automated Curation Using Pre-trained Models\n",
    "\n",
    "We'll use a pre-trained model to automatically curate our units. We'll use the toy tetrode model for this demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading pre-trained model for automated curation...\")\n",
    "model, model_info = sc.load_model(\n",
    "    repo_id=\"SpikeInterface/toy_tetrode_model\",\n",
    "    trusted=['numpy.dtype']\n",
    ")\n",
    "\n",
    "# Apply the model to our data\n",
    "print(\"Applying model to label units...\")\n",
    "labels = sc.auto_label_units(\n",
    "    sorting_analyzer=analyzer,\n",
    "    repo_id=\"SpikeInterface/toy_tetrode_model\",\n",
    "    trusted=['numpy.dtype']\n",
    ")\n",
    "\n",
    "print(\"\\nModel predictions:\")\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Visualizing Results\n",
    "\n",
    "Let's visualize some of our results to better understand the quality of our units. We'll create plots showing:\n",
    "1. Quality metrics distributions\n",
    "2. Unit templates for high and low SNR units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot quality metrics distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot firing rates\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(metrics['firing_rate'], bins=20)\n",
    "plt.xlabel('Firing Rate (Hz)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Firing Rate Distribution')\n",
    "\n",
    "# Plot SNR distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(metrics['snr'], bins=20)\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Count')\n",
    "plt.title('SNR Distribution')\n",
    "\n",
    "# Plot firing range distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(metrics['firing_range'], bins=20)\n",
    "plt.xlabel('Firing Range')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Firing Range Distribution')\n",
    "\n",
    "# Plot isolation distances\n",
    "plt.subplot(2, 2, 4)\n",
    "isolation_distances = metrics['isolation_distance'].dropna()  # Remove NaN values\n",
    "if len(isolation_distances) > 0:\n",
    "    plt.hist(isolation_distances, bins=20)\n",
    "    plt.xlabel('Isolation Distance')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Isolation Distance Distribution')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No isolation distance data\\n(requires multi-channel)', \n",
    "            ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    plt.title('Isolation Distance Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot templates for some units\n",
    "# Get units with highest and lowest SNR as examples\n",
    "good_unit = metrics['snr'].idxmax()\n",
    "bad_unit = metrics['snr'].idxmin()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "sw.plot_unit_templates(analyzer, unit_ids=[good_unit, bad_unit])\n",
    "plt.suptitle('Unit Templates (High SNR vs Low SNR)')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
